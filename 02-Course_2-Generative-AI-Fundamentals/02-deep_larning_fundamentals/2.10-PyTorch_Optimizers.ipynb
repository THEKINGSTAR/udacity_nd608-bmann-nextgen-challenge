{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch optimizers are important tools that help improve how a neural network learns from data by adjusting the model's parameters.\n",
    "\n",
    "By using these optimizers, like stochastic gradient descent (SGD) with momentum or Adam, we can quickly get started learning!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```Technical Terms Explained:```**\n",
    "\n",
    ">- Gradients: Directions and amounts by which a function increases most.\n",
    "The parameters can be changed in a direction opposite to the gradient of the loss function in order to reduce the loss.\n",
    "\n",
    ">- Learning Rate: This hyperparameter specifies how big the steps are when adjusting the neural network's settings during training.\n",
    "Too big, and you might skip over the best setting; too small, and it'll take a very long time to get there.\n",
    "\n",
    ">- Momentum: A technique that helps accelerate the optimizer in the right direction and dampens oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Examples\n",
    "Assuming ```model``` is your defined neural network.\n",
    "\n",
    "```lr=0.01``` sets the learning rate to ```0.01``` for either optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentum=0.9 smoothes out updates and can help training\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [PyTorch optimization tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)\n",
    "\n",
    "- [torch.optim.SGD documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)\n",
    "\n",
    "- [torch.optim.Adam documentation](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
    "\n",
    "- [Index of PyTorch optimizers](https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
